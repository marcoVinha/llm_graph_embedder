{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/llm_graph_embedder'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT_DIR = str(Path.cwd().parent)\n",
    "PROJECT_ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Gemma 2B-it model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2dc30c13d47b986e3bc45f01eab0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da6e98ad3c449a3b20b4cf0aa0a7792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 s, sys: 15.1 s, total: 49.3 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"cuda\")\n",
    "# model = AutoModel.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_data = pd.read_parquet(f\"{PROJECT_ROOT_DIR}/dataset/train.parquet\").sample(n=3)\n",
    "eval_data = pd.read_parquet(f\"{PROJECT_ROOT_DIR}/dataset/eval.parquet\").sample(n=1)\n",
    "test_data = pd.read_parquet(f\"{PROJECT_ROOT_DIR}/dataset/test.parquet\").sample(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'from': 'human', 'value': 'Given a citation graph: \\n{\\'edge_index\\': [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 2, 36, 8, 37, 0, 38, 39, 40, 41, 42, 43, 44, 45, 46, 0, 1, 2, 3, 8, 9, 0, 47, 48, 0, 49, 50, 51, 52, 14, 53, 33, 2, 1, 54, 55, 36, 56, 57, 54, 58, 59, 60, 61, 48, 62, 63, 30, 64], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]], \\'node_idx\\': 705, \\'node_list\\': [705, 2954, 13977, 40224, 86867, 109707, 121813, 122636, 148744, 150520, 83482, 36473, 147321, 149020, 11068, 95784, 85893, 110889, 134484, 87622, 99761, 53659, 106049, 139363, 132257, 40415, 53550, 120884, 20953, 43332, 6786, 28135, 92761, 80150, 131487, 97945, 156929, 14775, 59798, 4695, 67762, 104576, 105500, 67312, 131702, 144462, 168419, 114602, 123443, 30376, 83617, 126629, 152460, 75725, 13640, 124751, 142070, 11561, 36854, 40720, 125865, 90208, 42860, 110482, 124593]}\\nwhere the 0th node is the target paper, and other nodes are its one-hop or multi-hop neighbors, with the following information: \\nAbstract: Coded caching techniques have received significant attention lately due to their provable gains in reducing the cost of data delivery in wireless networks. These gains, however, have only been demonstrated under the assumption of a free placement phase. This unrealistic assumption poses a significant limitation, especially in cases where aggressive placement strategies can lead to a significant transmission cost that may even be higher than the corresponding cost of the delivery phase. In this paper, we relax this assumption and propose a general caching framework that captures the transmission cost of the two phases, and hence, results in minimizing the overall rate of the caching network. We model the dynamic nature of the network through a cost structure that allows for varying the network architecture and cost per transmission, across the placement and delivery phases. We start with the scenario where the individual users have no limit on the available caching memory and characterize the jointly optimal solution as a function of the different parameters in our cost structure. Then, we characterize the effect of memory constraints on the optimal solution in certain special cases. Interestingly, our results identify regions where the uncoded caching scheme outperforms its coded counterpart. Further, coded caching is shown to offer performance gains only when the network architecture during the placement phase is different from that during the delivery phase. \\n Title: towards jointly optimal placement and delivery to code or not to code in wireless caching networks \\n Question: Which arXiv CS sub-category does this paper belong to? Give the most likely arXiv CS sub-categories of this paper directly, in the form \"cs.XX\" with full name of the category.'},\n",
       "       {'from': 'gpt', 'value': 'cs.IT, Information Theory'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"formatted_conversations\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_conversations(sample):\n",
    "    prompt_template = \"\"\"<human>: {human_turn}\\n\\n<gpt>: {gpt_turn}\"\"\"\n",
    "\n",
    "    return prompt_template.format(\n",
    "        human_turn=sample[\"formatted_conversations\"][0][\"value\"],\n",
    "        gpt_turn=sample[\"formatted_conversations\"][1][\"value\"],\n",
    "    )\n",
    "\n",
    "def format_test_conversations(sample):\n",
    "    prompt_template = \"\"\"<human>: {human_turn}\\n\\n<gpt>: \"\"\"\n",
    "\n",
    "    return prompt_template.format(\n",
    "        human_turn=sample[\"formatted_conversations\"][0][\"value\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"full_conversation\"] = train_data.apply(format_training_conversations, axis=1)\n",
    "eval_data[\"full_conversation\"] = eval_data.apply(format_training_conversations, axis=1)\n",
    "\n",
    "test_data[\"full_conversation\"] = test_data.apply(format_test_conversations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train_data[['full_conversation']].reset_index(drop=True)),\n",
    "        \"eval\": Dataset.from_pandas(eval_data[['full_conversation']].reset_index(drop=True)),\n",
    "        \"test\": Dataset.from_pandas(test_data[['full_conversation']].reset_index(drop=True)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_conversation'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['full_conversation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_conversation'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Gemma in a couple of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b234cf1a2f7241fd93f30aa739f7a2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f0b59d9f0c4c2ab6341a8a6d9fd2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"full_conversation\"], padding=\"max_length\", truncation=True, max_length=1024\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_data = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_eval_data = dataset[\"eval\"].map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "tokenized_eval_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
