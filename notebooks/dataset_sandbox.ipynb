{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NodePropPredDataset(name=\"ogbn-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from umap.umap_ import UMAP\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def reduce_dimensions(vectors):\n",
    "    reducer = UMAP()\n",
    "    reducer.fit(vectors)\n",
    "    vectors = reducer.transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "\n",
    "    return x_vals, y_vals\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    random.seed(0)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        x_vals,\n",
    "        y_vals,\n",
    "        c=labels,\n",
    "        cmap=ListedColormap(plt.cm.tab20(np.linspace(0, 1, 20)).tolist() * 2),\n",
    "        s=10,  # Adjust size of points if needed\n",
    "        alpha=0.8,  # Adjust transparency for better visibility\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, ticks=np.arange(0, 40, step=1), label=\"Classes\")\n",
    "    plt.title(\"2D Embedding Visualization with Class Colors\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = reduce_dimensions(graph[\"node_feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(x, y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading text features\n",
    "\n",
    "OGB provides a file with all text features related to a papar (title and abstract): https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_features = pd.read_csv(\n",
    "    \"../titleabs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"paper_id\", \"title\", \"abstract\"],\n",
    "    index_col=\"paper_id\",\n",
    ")\n",
    "\n",
    "text_features = text_features.dropna()\n",
    "text_features.index = text_features.index.map(int)\n",
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx_to_paper_id = pd.read_csv(\n",
    "    \"./dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv\",\n",
    "    index_col=0,\n",
    "    names=[\"node_idx\", \"paper_id\"],\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    ")\n",
    "node_idx_to_paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features = pd.merge(\n",
    "    node_idx_to_paper_id,\n",
    "    text_features,\n",
    "    left_on=\"paper_id\",\n",
    "    right_on=text_features.index,\n",
    ")\n",
    "node_text_features.index.names = [\"node_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"publishing_year\"] = graph[\"node_year\"]\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"label\"] = labels\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows in node_text_features where publishing_year is 2018\n",
    "node_text_features[node_text_features[\"publishing_year\"] == 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_instructions = pd.read_json(\"hf://datasets/Jiabin99/Arxiv-PubMed-mix-NC-LP/arxiv_pub_node_st_cot_link_mix.json\")\n",
    "train_instructions = train_instructions[train_instructions['id'].str.startswith(\"arxiv_train\")]\n",
    "train_instructions[\"node_idx\"] = train_instructions[\"id\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "train_instructions.index = train_instructions[\"node_idx\"]\n",
    "train_instructions.index = train_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_instructions = load_dataset(\"Jiabin99/GraphGPT-eval-instruction\", split=\"test\", data_files={\"test\": \"arxiv_test_instruct_std.json\"})\n",
    "eval_instructions = pd.DataFrame(eval_instructions)\n",
    "eval_instructions[\"node_idx\"] = eval_instructions[\"id\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "eval_instructions.index = eval_instructions[\"node_idx\"]\n",
    "eval_instructions.index = eval_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging text features and instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `train_instructions` on the indexes of both dataframes\n",
    "train_data = pd.merge(node_text_features, train_instructions, left_index=True, right_index=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_data[\"publishing_year\"]), max(train_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `eval_insstructions` on the indexes of both dataframes\n",
    "eval_data = pd.merge(node_text_features, eval_instructions, left_index=True, right_index=True)\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(eval_data[\"publishing_year\"]), max(eval_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating full set of instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_graph_in_conversations(conversation, graph_data):\n",
    "    parsed_human_message = conversation[0][\"value\"]\n",
    "    parsed_human_message = parsed_human_message.replace(\"<graph>\", str(graph_data))\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": parsed_human_message\n",
    "        },\n",
    "        conversation[1]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['formatted_conversations'] = train_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row['conversations'], row['graph']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data['formatted_conversations'] = eval_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row['conversations'], row['graph']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Gemma 2B-it model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.model_download(\"google/gemma/pyTorch/2b-it\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
