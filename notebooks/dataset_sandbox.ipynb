{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading text features for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT_DIR = str(Path.cwd().parent)\n",
    "PROJECT_ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv.gz and puts into PROJECT_ROOT_DIR\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv.gz\"\n",
    "filename = Path(PROJECT_ROOT_DIR) / \"titleabs.tsv.gz\"\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to download file.\")\n",
    "else:\n",
    "    print(\"File already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "with gzip.open(filename, \"rb\") as f_in:\n",
    "    with open(str(filename).replace(\".gz\", \"\"), \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NodePropPredDataset(name=\"ogbn-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from umap.umap_ import UMAP\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def reduce_dimensions(vectors):\n",
    "    reducer = UMAP()\n",
    "    reducer.fit(vectors)\n",
    "    vectors = reducer.transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "\n",
    "    return x_vals, y_vals\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    random.seed(0)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        x_vals,\n",
    "        y_vals,\n",
    "        c=labels,\n",
    "        cmap=ListedColormap(plt.cm.tab20(np.linspace(0, 1, 20)).tolist() * 2),\n",
    "        s=10,  # Adjust size of points if needed\n",
    "        alpha=0.8,  # Adjust transparency for better visibility\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, ticks=np.arange(0, 40, step=1), label=\"Classes\")\n",
    "    plt.title(\"2D Embedding Visualization with Class Colors\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = reduce_dimensions(graph[\"node_feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_embeddings(x, y, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading text features\n",
    "\n",
    "OGB provides a file with all text features related to a papar (title and abstract): https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx_2_paper_id_f_name = \"./dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv\"\n",
    "\n",
    "if not os.path.isfile(node_idx_2_paper_id_f_name):\n",
    "    with gzip.open(node_idx_2_paper_id_f_name + \".gz\", \"rb\") as f_in:\n",
    "        with open(filename, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_features = pd.read_csv(\n",
    "    \"../titleabs.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"paper_id\", \"title\", \"abstract\"],\n",
    "    index_col=\"paper_id\",\n",
    ")\n",
    "\n",
    "text_features = text_features.dropna()\n",
    "text_features.index = text_features.index.map(int)\n",
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx_to_paper_id = pd.read_csv(\n",
    "    node_idx_2_paper_id_f_name,\n",
    "    index_col=0,\n",
    "    names=[\"node_idx\", \"paper_id\"],\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    ")\n",
    "node_idx_to_paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features = pd.merge(\n",
    "    node_idx_to_paper_id,\n",
    "    text_features,\n",
    "    left_on=\"paper_id\",\n",
    "    right_on=text_features.index,\n",
    ")\n",
    "node_text_features.index.names = [\"node_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"publishing_year\"] = graph[\"node_year\"]\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"label\"] = labels\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows in node_text_features where publishing_year is 2018\n",
    "node_text_features[node_text_features[\"publishing_year\"] == 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_instructions = pd.read_json(\n",
    "    \"hf://datasets/Jiabin99/Arxiv-PubMed-mix-NC-LP/arxiv_pub_node_st_cot_link_mix.json\"\n",
    ")\n",
    "train_instructions = train_instructions[\n",
    "    train_instructions[\"id\"].str.startswith(\"arxiv_train\")\n",
    "]\n",
    "train_instructions[\"node_idx\"] = train_instructions[\"id\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "train_instructions.index = train_instructions[\"node_idx\"]\n",
    "train_instructions.index = train_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_instructions = load_dataset(\n",
    "    \"Jiabin99/GraphGPT-eval-instruction\",\n",
    "    split=\"test\",\n",
    "    data_files={\"test\": \"arxiv_test_instruct_std.json\"},\n",
    ")\n",
    "eval_instructions = pd.DataFrame(eval_instructions)\n",
    "eval_instructions[\"node_idx\"] = eval_instructions[\"id\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "eval_instructions.index = eval_instructions[\"node_idx\"]\n",
    "eval_instructions.index = eval_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging text features and instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `train_instructions` on the indexes of both dataframes\n",
    "train_data = pd.merge(\n",
    "    node_text_features, train_instructions, left_index=True, right_index=True\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_data[\"publishing_year\"]), max(train_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `eval_insstructions` on the indexes of both dataframes\n",
    "eval_data = pd.merge(\n",
    "    node_text_features, eval_instructions, left_index=True, right_index=True\n",
    ")\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(eval_data[\"publishing_year\"]), max(eval_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating full set of instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_graph_in_conversations(conversation, graph_data):\n",
    "    parsed_human_message = conversation[0][\"value\"]\n",
    "    parsed_human_message = parsed_human_message.replace(\"<graph>\", str(graph_data))\n",
    "\n",
    "    return [{\"from\": \"human\", \"value\": parsed_human_message}, conversation[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"formatted_conversations\"] = train_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row[\"conversations\"], row[\"graph\"]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[\"formatted_conversations\"] = eval_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row[\"conversations\"], row[\"graph\"]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Gemma 2B-it model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "# model = AutoModel.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_text = \"\"\"\n",
    "<human>: List all 40 sub-categories of the 'Computer Science' category in the ArXiv dataset.\n",
    "<gpt>: Sure! Here are all the 40 sub-categories of the 'Computer Science' category in the ArXiv dataset:\n",
    "\"\"\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=1024)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune Gemma in a couple of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_template = \"\"\"<human>: {}\\n\\n<gpt>: {}\"\"\"\n",
    "\n",
    "train_prompts_dicts = train_data.sample(n=3, random_state=0)[\n",
    "    \"formatted_conversations\"\n",
    "].tolist()\n",
    "train_prompts = []\n",
    "\n",
    "for prompt in train_prompts_dicts:\n",
    "    train_prompts.append(\n",
    "        train_prompt_template.format(prompt[0][\"value\"], prompt[1][\"value\"])\n",
    "    )\n",
    "\n",
    "train_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert your list of prompts into a dictionary\n",
    "fine_tuning_data = {\"text\": train_prompts}\n",
    "\n",
    "# Create a Dataset object\n",
    "fine_tuning_dataset = Dataset.from_dict(fine_tuning_data)\n",
    "fine_tuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.__class__.__name__)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_fine_tuning_dataset = fine_tuning_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_fine_tuning_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "\n",
    "train_test_split = tokenized_fine_tuning_dataset.train_test_split(test_size=(1 / 3))\n",
    "train_fine_tuning_dataset = train_test_split[\"train\"]\n",
    "eval_fine_tuning_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False  # Set mlm=False for causal language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_fine_tuning_dataset,\n",
    "    eval_dataset=eval_fine_tuning_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "del eval_data\n",
    "del eval_instructions\n",
    "del train_instructions\n",
    "del node_text_features\n",
    "del node_idx_to_paper_id\n",
    "del text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Train runned succesfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
