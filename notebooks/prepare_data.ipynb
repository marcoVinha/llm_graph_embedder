{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT_DIR = str(Path.cwd().parent)\n",
    "PROJECT_ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "\n",
    "dataset = NodePropPredDataset(name=\"ogbn-arxiv\")\n",
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading nodes textual features\n",
    "\n",
    "OGB provides a file with all text features related to a papar (title and abstract): https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv.gz and puts into PROJECT_ROOT_DIR\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "url = \"https://snap.stanford.edu/ogb/data/misc/ogbn_arxiv/titleabs.tsv.gz\"\n",
    "text_features_filename = Path(PROJECT_ROOT_DIR) / \"titleabs.tsv\"\n",
    "\n",
    "if not os.path.isfile(str(text_features_filename) + \".gz\"):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(str(text_features_filename) + \".gz\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"Failed to download file.\")\n",
    "else:\n",
    "    print(\"File already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "with gzip.open(str(text_features_filename) + \".gz\", \"rb\") as f_in:\n",
    "    with open(text_features_filename, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx_2_paper_id_f_name = \"./dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv\"\n",
    "\n",
    "if not os.path.isfile(node_idx_2_paper_id_f_name):\n",
    "    with gzip.open(node_idx_2_paper_id_f_name + \".gz\", \"rb\") as f_in:\n",
    "        with open(node_idx_2_paper_id_f_name, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text_features = pd.read_csv(\n",
    "    text_features_filename,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    "    names=[\"paper_id\", \"title\", \"abstract\"],\n",
    "    index_col=\"paper_id\",\n",
    ")\n",
    "\n",
    "text_features = text_features.dropna()\n",
    "text_features.index = text_features.index.map(int)\n",
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx_to_paper_id = pd.read_csv(\n",
    "    node_idx_2_paper_id_f_name,\n",
    "    index_col=0,\n",
    "    names=[\"node_idx\", \"paper_id\"],\n",
    "    header=None,\n",
    "    skiprows=1,\n",
    ")\n",
    "node_idx_to_paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features = pd.merge(\n",
    "    node_idx_to_paper_id,\n",
    "    text_features,\n",
    "    left_on=\"paper_id\",\n",
    "    right_on=text_features.index,\n",
    ")\n",
    "node_text_features.index.names = [\"node_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"publishing_year\"] = graph[\"node_year\"]\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_text_features[\"label\"] = labels\n",
    "node_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows in node_text_features where publishing_year is 2018\n",
    "node_text_features[node_text_features[\"publishing_year\"] == 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "train_instructions = pd.read_json(\n",
    "    \"hf://datasets/Jiabin99/Arxiv-PubMed-mix-NC-LP/arxiv_pub_node_st_cot_link_mix.json\"\n",
    ")\n",
    "train_instructions = train_instructions[\n",
    "    train_instructions[\"id\"].str.startswith(\"arxiv_train\")\n",
    "]\n",
    "train_instructions[\"node_idx\"] = train_instructions[\"id\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "train_instructions.index = train_instructions[\"node_idx\"]\n",
    "train_instructions.index = train_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_instructions = load_dataset(\n",
    "    \"Jiabin99/GraphGPT-eval-instruction\",\n",
    "    split=\"test\",\n",
    "    data_files={\"test\": \"arxiv_test_instruct_std.json\"},\n",
    ")\n",
    "test_instructions = pd.DataFrame(test_instructions)\n",
    "test_instructions[\"node_idx\"] = test_instructions[\"id\"].apply(\n",
    "    lambda x: x.split(\"_\")[-1]\n",
    ")\n",
    "test_instructions.index = test_instructions[\"node_idx\"]\n",
    "test_instructions.index = test_instructions.index.map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging text features and instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `train_instructions` on the indexes of both dataframes\n",
    "train_data = pd.merge(\n",
    "    node_text_features, train_instructions, left_index=True, right_index=True\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(train_data[\"publishing_year\"]), max(train_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that makes an inner join between `node_text_features` and `test_instructions` on the indexes of both dataframes\n",
    "test_data = pd.merge(\n",
    "    node_text_features, test_instructions, left_index=True, right_index=True\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_data[\"publishing_year\"]), max(test_data[\"publishing_year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating full set of instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_graph_in_conversations(conversation, graph_data):\n",
    "    parsed_human_message = conversation[0][\"value\"]\n",
    "    parsed_human_message = parsed_human_message.replace(\"<graph>\", str(graph_data))\n",
    "\n",
    "    return [{\"from\": \"human\", \"value\": parsed_human_message}, conversation[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"formatted_conversations\"] = train_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row[\"conversations\"], row[\"graph\"]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"formatted_conversations\"] = test_data.apply(\n",
    "    lambda row: replace_graph_in_conversations(row[\"conversations\"], row[\"graph\"]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting eval data from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = train_data[train_data[\"publishing_year\"] == 2017]\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data[\"publishing_year\"] != 2017]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $PROJECT_ROOT_DIR/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_parquet(f\"{PROJECT_ROOT_DIR}/dataset/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data.to_parquet(f\"{PROJECT_ROOT_DIR}/dataset/eval.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_parquet(f\"{PROJECT_ROOT_DIR}/dataset/test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
