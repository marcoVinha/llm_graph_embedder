{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT_DIR = str(Path.cwd().parent)\n",
    "PROJECT_ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_parquet(f\"{PROJECT_ROOT_DIR}/dataset/test.parquet\").sample(\n",
    "    n=4000, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_conversations(sample):\n",
    "    prompt_template = \"\"\"<human>: {human_turn}\\n\\n<gpt>:\"\"\"\n",
    "\n",
    "    return prompt_template.format(\n",
    "        human_turn=sample[\"formatted_conversations\"][0][\"value\"],\n",
    "        gpt_turn=sample[\"formatted_conversations\"][1][\"value\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"full_conversation\"] = test_data.apply(format_test_conversations, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"test\": Dataset.from_pandas(\n",
    "            test_data[[\"full_conversation\"]].reset_index(drop=True)\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model with LoRA configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gemma 2 2B-it as `AutoModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(sample):\n",
    "    # Tokenize the input sentences\n",
    "    inputs = tokenizer(\n",
    "        sample, padding=True, truncation=True, max_length=3000, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move inputs to GPU if available\n",
    "    device = torch.device(\"cuda\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Pooling: Mean pooling over the sequence length\n",
    "        embeddings = hidden_states.mean(dim=1)\n",
    "\n",
    "    # Convert embeddings to CPU and numpy for storage\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_data[\"embeddings\"] = test_data[[\"full_conversation\"]].map(generate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def plot_embeddings(x_vals, y_vals, labels):\n",
    "    random.seed(0)\n",
    "\n",
    "    # Generate 40 distinct colors using a custom colormap\n",
    "    cmap = ListedColormap(\n",
    "        plt.cm.tab20(np.linspace(0, 1, 20)).tolist() * 2\n",
    "    )  # Extends tab20 to 40 colors\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        x_vals,\n",
    "        y_vals,\n",
    "        c=labels,\n",
    "        cmap=cmap,  # Custom colormap with 40 colors\n",
    "        s=10,  # Adjust size of points\n",
    "        alpha=0.8,  # Transparency\n",
    "    )\n",
    "\n",
    "    # Add a colorbar to show the mapping of colors to classes\n",
    "    cbar = plt.colorbar(scatter, ticks=np.arange(0, 40, step=1))\n",
    "    cbar.set_label(\"Classes\")\n",
    "    plt.title(\"2D Embedding Visualization with Class Colors\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import UMAP\n",
    "\n",
    "\n",
    "def reduce_dimensions(values):\n",
    "    vectors = np.asarray(values)\n",
    "\n",
    "    reducer = UMAP()\n",
    "    reducer.fit(vectors)\n",
    "    vectors = reducer.transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_coord, y_coord = reduce_dimensions(\n",
    "    np.vstack(test_data[\"embeddings\"].values), test_data[\"label\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(x_coord, y_coord, test_data[\"label\"].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
